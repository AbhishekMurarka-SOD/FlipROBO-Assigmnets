Q1.The value of correlation coefficient will always be: (C) between -1 and 1.
Q2.Which of the following cannot be used for dimensionality reduction: (C) Recursive feature elimination
Q3.Which of the following is not a kernel in Support Vector Machines: B) Radial Basis Function
Q4.Amongst the following, which one is least suitable for a dataset having non-linear decision boundaries: D) Support Vector Classifier
Q5.In a Linear Regression problem, ‘X’ is independent variable and ‘Y’ is dependent variable, where ‘X’ represents
weight in pounds. If you convert the unit of ‘X’ to kilograms, then new coefficient of ‘X’ will be: B) same as old coefficient of ‘X’
Q6.As we increase the number of estimators in ADABOOST Classifier, what happens to the accuracy of the model: B) increases
Q7.Which of the following is not an advantage of using random forest instead of decision trees: C) Random Forests are easy to interpret

Q8.Which of the following are correct about Principal Components: A, B
Q9.Which of the following are applications of clustering: C) Identifying spam or ham emails
Q10.Which of the following is(are) hyper parameters of a decision tree: A,B,C,D

Q11.What are outliers? Explain the Inter Quartile Range(IQR) method for outlier detection.
A Vlaue that lies outside the range of other observation on data, in other words outliers is an observation that diverges from an overall pattern on sample.
The IQR is calculated as the difference between the 75th and 25th percentiles of the data and defines the box in a whisker plot. 
The IQR can be used to identify outliers by defining limits on the sample values that are a factor k of the IQR below the 25th percentile or above the 75th percentile.

Q12.What is the primary difference between bagging and boosting algorithms?
1)Bagging is simplest way of combining predictions that belong to the same type, whereas Boosting is a way of combining predictions that belong to the different types.
2)In Bagging each model receives equal weight, whereas in Boosting models are weighted according to their performance.
3)Bagging tries to solve over-fitting problem whereas Boosting tries to reduce bias.
4)If the classifier is unstable (high variance), then apply bagging, else If the classifier is stable and simple (high bias) the apply boosting.
5)Bagging tries to solve over-fitting problem and Boosting tries to reduce bias.

Q13)What is adjusted R2 in logistic regression. How is it calculated?
The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. 
The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. 
It decreases when a predictor improves the model by less than expected by chance. The adjusted R-squared can be negative, but it’s usually not.  
It is always lower than the R-squared.
Adjusted R-squared value can be calculated based on value of r-squared, number of independent variables (predictors), total sample size. Every time we add a independent variable to a model, the R-squared increases, even if the independent variable is insignificant.

Q14)What is the difference between standardisation and normalisation?
Normalization often also simply called Min-Max scaling basically shrinks the range of the data such that the range is fixed between 0 and 1 (or -1 to 1 if there are negative values). It works better for cases in which the standardization might not work so well. If the distribution is not Gaussian or the standard deviation is very small, the min-max scaler works better.
Standardization (or Z-score normalization) is the process of rescaling the features so that they’ll have the properties of a Gaussian distribution with μ=0 and σ=1.

Q15)What is cross-validation? Describe one advantage and one disadvantage of using cross-validation?
Cross-validation is a technique for evaluating ML models by training several ML models on subsets of the available input data and evaluating them on the complementary subset of the data. In k-fold cross-validation, you split the input data into k subsets of data (also known as folds). 
The advantage of this method is that the proportion of the validation or training split is not dependent on the number of folds (K-fold test). However, there is a disadvantage as well. There are chances that you might miss out some observations whereas you might select some observations more than once.


